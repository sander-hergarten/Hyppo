from typing import Any
import tensorflow as tf
import pandas as pd
import time


from collections import deque
from sqlalchemy.engine import create_engine
from google.cloud import bigquery
from tf_agents.trajectories import time_step as ts
from tf_agents.trajectories import Trajectory
from uuid import uuid1
import numpy as np


from pandas_to_bq import BigQueryPandasUploader


class StepSaver:
    def __init__(
        self,
        batch_size,
        environment_name="Coinrun",
        dataset=None,
        extend_length: int = 1001,
    ):

        self.environment_name = environment_name

        self.engine = create_engine("bigquery://deplearn")
        self.batch_size = batch_size

        self._reset_queues()

        self.dataset = (
            f"dataset_run_{int(time.time() *1000)}" if not dataset else dataset
        )

        self.extend_length = extend_length

        print(f"generating dataset: {self.dataset}")

        with self.engine.connect() as conn:
            conn.execute(f"CREATE SCHEMA {self.dataset}")

        schema = [
            bigquery.SchemaField(
                "action", bigquery.enums.SqlTypeNames.INTEGER, mode="REPEATED"
            ),
            bigquery.SchemaField(
                "observation", bigquery.enums.SqlTypeNames.STRING, mode="REPEATED"
            ),
            bigquery.SchemaField(
                "discount", bigquery.enums.SqlTypeNames.FLOAT, mode="REPEATED"
            ),
            bigquery.SchemaField(
                "step_type", bigquery.enums.SqlTypeNames.INTEGER, mode="REPEATED"
            ),
            bigquery.SchemaField(
                "next_step_type", bigquery.enums.SqlTypeNames.INTEGER, mode="REPEATED"
            ),
            bigquery.SchemaField(
                "reward", bigquery.enums.SqlTypeNames.FLOAT, mode="REPEATED"
            ),
        ]

        self.uploader = BigQueryPandasUploader(
            schema=schema, project_id="deplearn", dataset=self.dataset
        )
        # with self.engine.connect() as conn:
        #     conn.execute(
        #         f"CREATE TABLE {self.dataset}.observations (step int, episode_id string, environment int, observation bytes)"
        #     )

        #     conn.execute(
        #         (
        #             f"CREATE TABLE {self.dataset}.step_data "
        #             "(step int, "
        #             " episode_id string,"
        #             " environment int,"
        #             " action int,"
        #             " step_type int,"
        #             " next_step_type int,"
        #             " reward numeric,"
        #             " discount numeric)"
        #         )
        #     )

        self._helper_loop_trajectory = lambda batch_trajectory, keyword: tf.unstack(
            getattr(batch_trajectory, keyword)
        )
        self._np_to_base64 = (
            lambda image: tf.io.encode_base64(
                tf.io.encode_jpeg(tf.cast(image, tf.uint8))
            )
            .numpy()
            .decode("utf-8")
        )

        self.processing_queue = []

    def add_data_to_queue(self, batch_trajectory: Trajectory):
        """adds Data the the internal queue object. Can be used as an observer callback

        Function takes the batched trajectory datat and adds it to a list filled with queue objects where the index represents the environment id.
        After every episode the data is

        Args:
            batch_trajectory (Trajectory): the batched trajectory data returned from a Tf agents driver.
        """

        trajectories = self.loop_trajectory(batch_trajectory)

        # loops through all environments steps.
        for environment_id, (queue, trajectory) in enumerate(
            zip(self.queues, trajectories)
        ):
            queue.append(trajectory)

            if trajectory.step_type != ts.StepType.LAST:
                continue

            print("episode done")

            # Exectutes when Last Step of episode occurs
            queue_copy = queue.copy()
            queue.clear()

            observation = self.write_episode_entry(environment_id, queue_copy)

            self.processing_queue.append(observation)

    def loop_trajectory(self, batch_trajectory: Trajectory) -> list:
        """transforms the batched trajectories into a more usefull form.

        Trajectories generated by the environment have the form:
            (keyword_1: Tensor(batch_size), keyword_2: Tensor(batch_size), ...)
        This function changes the shape to:
            [
                (keyword_1: x_0, keyword_2: y_0, ...),
                (keyword_2: x_1, keyword_2: y_1, ...),
                ...
                (keyword_19: x_1, keyword_2: y_19, ...),
            ]


        Args:
            batch_trajectory (Trajectory): the Batched trajectory data

        Returns:
            list of Trajectory info
        """

        trajectory_data = [
            self._helper_loop_trajectory(batch_trajectory, attr)
            if attr != "policy_info"
            else [batch_trajectory.policy_info for _ in range(self.batch_size)]
            for attr in [
                "step_type",
                "observation",
                "action",
                "policy_info",
                "next_step_type",
                "reward",
                "discount",
            ]
        ]

        return [Trajectory(*traj) for traj in np.array(trajectory_data).T]

    def commit_episodes(self):
        """Function to commit episodes into the bigquery db

        Deploys a Thread that runs in the background to not block main Thread execution
        """
        # Thread(target=self._commit_episode).start()
        self._commit_episode()
        self._reset_queues()

    def _commit_episode(self):
        """Function that commits the step data to the bigquery db. Designed to run on a secondary Thread.

        Function groups the queued data into episodes before commiting and uses the pandas gbq library for bigquery interaction.
        """

        df = pd.DataFrame(self.processing_queue)

        # for ellement in self.processing_queue:
        #     print(len(ellement["action"]))
        #     print(len(ellement["step_type"]))
        #     print(len(ellement["observation"]))
        #     print(len(ellement["next_step_type"]))
        #     print(len(ellement["reward"]))

        self.uploader.upload(df, table_id="Coinrun")

        self.processing_queue = []

        # pandas_gbq.to_gbq(
        #     df,
        #     f"{self.dataset}.{self.environment_name}_observations",
        #     project_id="deplearn",
        #     if_exists="append",
        # )
        print("commited data to bigquery")

    def write_episode_entry(
        self, environment_id: int, queue: deque
    ) -> list[dict[str, Any]]:
        """[TODO:summary]

        [TODO:description]

        Args:
            environment_id:
            queue: [TODO:description]
        """
        episode_length = len(queue)
        episode_id = str(uuid1())

        entry = {
            "episode_id": episode_id,
            "environment": environment_id,
            "action": [0 for _ in range(self.extend_length)],
            "step_type": [0 for _ in range(self.extend_length)],
            "next_step_type": [0 for _ in range(self.extend_length)],
            "reward": [0 for _ in range(self.extend_length)],
            "discount": [0 for _ in range(self.extend_length)],
            "observation": ["" for _ in range(self.extend_length)],
        }

        for n in range(episode_length):
            trajectory = queue.pop()

            entry["action"][n] = trajectory.action.numpy()
            entry["step_type"][n] = trajectory.step_type.numpy()
            entry["next_step_type"][n] = trajectory.next_step_type.numpy()
            entry["reward"][n] = trajectory.reward.numpy()
            entry["discount"][n] = trajectory.discount.numpy()
            entry["observation"][n] = self._np_to_base64(trajectory.observation)

        # for column in [
        #     "action",
        #     "step_type",
        #     "next_step_type",
        #     "reward",
        #     "discount",
        #     "observation",
        # ]:
        #     print(f"{column}  =  {type(entry[column][0])}")

        return entry

    def _reset_queues(self):
        self.queues = [deque([]) for _ in range(self.batch_size)]
