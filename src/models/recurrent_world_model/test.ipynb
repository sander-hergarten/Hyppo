{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from sequence_model import DynamicsPredictor, SequenceModel, Encoder\n",
    "from auxilary_output import Decoder, RewardPredictor, ContinuePredictor\n",
    "from rlu_unplugged_transform import batch_dataset\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "continue_predictor = ContinuePredictor()\n",
    "\n",
    "out =continue_predictor(tf.zeros((1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert 0.0 to EagerTensor of dtype int32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m continue_predictor\u001b[39m.\u001b[39;49mbinary_crossentropy(tf\u001b[39m.\u001b[39;49mzeros((\u001b[39m1\u001b[39;49m),dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32), tf\u001b[39m.\u001b[39;49mcast(out[\u001b[39m0\u001b[39;49m][\u001b[39m0\u001b[39;49m], dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearningv3/lib/python3.10/site-packages/keras/losses.py:152\u001b[0m, in \u001b[0;36mLoss.__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     call_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    149\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    150\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m losses \u001b[39m=\u001b[39m call_fn(y_true, y_pred)\n\u001b[1;32m    154\u001b[0m in_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(y_pred)\n\u001b[1;32m    155\u001b[0m out_mask \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39mget_mask(losses)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearningv3/lib/python3.10/site-packages/keras/losses.py:284\u001b[0m, in \u001b[0;36mLossFunctionWrapper.call\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    277\u001b[0m     y_pred, y_true \u001b[39m=\u001b[39m losses_utils\u001b[39m.\u001b[39msqueeze_or_expand_dimensions(\n\u001b[1;32m    278\u001b[0m         y_pred, y_true\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    281\u001b[0m ag_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn, tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    283\u001b[0m )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m ag_fn(y_true, y_pred, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fn_kwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearningv3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearningv3/lib/python3.10/site-packages/keras/losses.py:2166\u001b[0m, in \u001b[0;36mbinary_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing, axis)\u001b[0m\n\u001b[1;32m   2164\u001b[0m y_pred \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(y_pred)\n\u001b[1;32m   2165\u001b[0m y_true \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(y_true, y_pred\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m-> 2166\u001b[0m label_smoothing \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(label_smoothing, dtype\u001b[39m=\u001b[39;49my_pred\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m   2168\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_smooth_labels\u001b[39m():\n\u001b[1;32m   2169\u001b[0m     \u001b[39mreturn\u001b[39;00m y_true \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m label_smoothing) \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m label_smoothing\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert 0.0 to EagerTensor of dtype int32"
     ]
    }
   ],
   "source": [
    "\n",
    "continue_predictor.binary_crossentropy(tf.zeros((1),dtype=tf.int32), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamics_predictor = DynamicsPredictor()\n",
    "sequence_model = SequenceModel()\n",
    "encoder = Encoder()\n",
    "\n",
    "decoder = Decoder()\n",
    "reward_predictor = RewardPredictor()\n",
    "continue_predictor = ContinuePredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "kl_divergence = tf.keras.losses.KLDivergence()\n",
    "\n",
    "recurrent_state = tf.zeros((1, 256))\n",
    "stochastic_state = tf.zeros((1, 1024))\n",
    "\n",
    "action = tf.stack([tf.one_hot(0, 16)])\n",
    "observation = tf.zeros((1, 64, 64, 3))\n",
    "reward = tf.zeros((1, 1))\n",
    "continue_flag = tf.zeros((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/homebrew/Caskroom/miniconda/base/envs/deeplearningv3/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/sanderhergarten/Documents/programming/Hyppo/src/models/recurrent_world_model/utils.py\", line 37, in symlog_loss  *\n        return 0.5 * (y_pred - symlog(y_true)) ** 2\n\n    ValueError: Dimensions must be equal, but are 79 and 64 for '{{node sub}} = Sub[T=DT_FLOAT](y_pred, mul)' with input shapes: [1,79,79,192], [1,64,64,3].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m loss_representation \u001b[39m=\u001b[39m kl_divergence(tf\u001b[39m.\u001b[39mstop_gradient(res), stochastic_dist)\n\u001b[1;32m      9\u001b[0m model_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([recurrent_state, stochastic_state], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m decoder\u001b[39m.\u001b[39;49mloss_fn(model_state, observation)\n\u001b[1;32m     11\u001b[0m \u001b[39m# loss_decoder = -tf.math.log(decoder.loss(model_state, observation))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# loss_reward = -tf.math.log(reward_predictor.loss(model_state, reward))\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# loss_continue = -tf.math.log(continue_predictor.loss(model_state, continue_flag))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mloss_dynamics \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m\u001b[39m*\u001b[39mloss_representation \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m*\u001b[39m loss_decoder\n",
      "File \u001b[0;32m~/Documents/programming/Hyppo/src/models/recurrent_world_model/auxilary_output.py:133\u001b[0m, in \u001b[0;36mDecoder.loss_fn\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    130\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_no_symexp(x)\n\u001b[1;32m    131\u001b[0m processed_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresizing_layer(y)\n\u001b[0;32m--> 133\u001b[0m k \u001b[39m=\u001b[39m symlog_loss(processed_y, y_pred)\n\u001b[1;32m    135\u001b[0m \u001b[39mreturn\u001b[39;00m k\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/deeplearningv3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/f8/gyd0q7m15jbg_r5chc34ntdm0000gn/T/__autograph_generated_file_7wuzhel.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__symlog_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (ag__\u001b[39m.\u001b[39mld(y_pred) \u001b[39m-\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(symlog), (ag__\u001b[39m.\u001b[39mld(y_true),), \u001b[39mNone\u001b[39;00m, fscope)) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/sanderhergarten/Documents/programming/Hyppo/src/models/recurrent_world_model/utils.py\", line 37, in symlog_loss  *\n        return 0.5 * (y_pred - symlog(y_true)) ** 2\n\n    ValueError: Dimensions must be equal, but are 79 and 64 for '{{node sub}} = Sub[T=DT_FLOAT](y_pred, mul)' with input shapes: [1,79,79,192], [1,64,64,3].\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    recurrent_state = sequence_model(recurrent_state=recurrent_state, stochastic_state=stochastic_state, action=action)[0]\n",
    "    stochastic_dist = encoder.distribution(recurrent_state, observation)\n",
    "    res = dynamics_predictor.distribution(recurrent_state)\n",
    "\n",
    "    loss_dynamics = kl_divergence(tf.stop_gradient(stochastic_dist), res)\n",
    "    loss_representation = kl_divergence(tf.stop_gradient(res), stochastic_dist)\n",
    "    \n",
    "    model_state = tf.concat([recurrent_state, stochastic_state], axis=1)\n",
    "    decoder.loss_fn(model_state, observation)\n",
    "    # loss_decoder = -tf.math.log(decoder.loss(model_state, observation))\n",
    "    # loss_reward = -tf.math.log(reward_predictor.loss(model_state, reward))\n",
    "    # loss_continue = -tf.math.log(continue_predictor.loss(model_state, continue_flag))\n",
    "\n",
    "    loss = 0.5*loss_dynamics + 0.1*loss_representation + 1 * loss_decoder\n",
    "\n",
    "grad_var_pairs = []\n",
    "\n",
    "def _add_grad_var_pair(grad, var):\n",
    "    grad_var_pairs.extend(list(zip(grad, var)))\n",
    "\n",
    "\n",
    "for model in [dynamics_predictor, \n",
    "              encoder, \n",
    "              sequence_model, \n",
    "              decoder, \n",
    "              reward_predictor, \n",
    "              continue_predictor]:\n",
    "    variables = model.trainable_variables\n",
    "    grads = tape.gradient(loss, variables)\n",
    "\n",
    "    _add_grad_var_pair(grads, variables)\n",
    "\n",
    "optimizer.apply_gradients(grad_var_pairs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 19:14:46.224729: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "data = batch_dataset(tfds.load(\"rlu_atari\", split=\"train[:5%]\"))\n",
    "k = WorldModel2()\n",
    "k.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "k.fit(data, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SequenceModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.recurrent_state_size = config[\"model_parameters\"][\"recurrent_state_size\"]\n",
    "\n",
    "        self.mlp_layer_size = config[\"model_sizes\"][\"mlp_layer_size\"]\n",
    "        self.gru_recurrent_units = config[\"model_sizes\"][\"gru_recurrent_units\"]\n",
    "        self.concat_layer = layers.Concatenate()\n",
    "\n",
    "        self.gru_layers = [\n",
    "            layers.GRUCell(units, \"relu\") for units in self.gru_recurrent_units\n",
    "        ]\n",
    "\n",
    "        self.mlp_layers = [\n",
    "            layers.Dense(layer_size, \"relu\") for layer_size in self.mlp_layer_size\n",
    "        ]\n",
    "\n",
    "        self.output_layer = layers.Dense(self.recurrent_state_size, \"sigmoid\")\n",
    "\n",
    "    def call(self, recurrent_state, stochastic_state, action):\n",
    "        concatinated_inputs = self.concat_layer([stochastic_state, action])\n",
    "\n",
    "        intermediate_recurrent = concatinated_inputs\n",
    "\n",
    "        state = recurrent_state\n",
    "\n",
    "        for layer in self.gru_layers:\n",
    "            intermediate_recurrent, state = layer(intermediate_recurrent, state)\n",
    "\n",
    "        intermediate_dense = intermediate_recurrent\n",
    "\n",
    "        for layer in self.mlp_layers:\n",
    "            intermediate_dense = layer(intermediate_dense)\n",
    "\n",
    "        output = self.output_layer(intermediate_dense)\n",
    "\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearningv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9347489c2696a86b815da7667aa234b0619fdc69ff0b41ca895c9aa33925a28d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
